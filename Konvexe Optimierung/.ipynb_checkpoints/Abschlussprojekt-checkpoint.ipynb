{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abschlussprojekt Konvexe Optimierung\n",
    "\n",
    "## Support-Vektor-Maschine\n",
    "\n",
    "##### Beschreibung:\n",
    "Ziel dieses Projekts ist es, einen Solver für eine Support-Vektor-Machine (SVM) für Klassifikation zu implementieren.\n",
    "In den bisherigen Übungsaufgaben waren die Beispiele hauptsächlich Regressionsbeispiele, nun geht es um Klassifikation. \n",
    "\n",
    "Als Eingabe gibt es eine Menge von $k$ zweidimensionalen Punkten $P = \\{x_1, x_2, \\dots, x_k\\}$ mit $x_i \\in \\mathbb{R}^2$. jeder dieser Punkte hat ein Label $y_i$ (rot oder blau) bzw. $y_i \\in \\{+1, -1\\}$, das angibt, zu welcher Klasse der Punkt gehört. Ziel ist es eine Gerade zu finden, die beide Klassen von Punkten möglichst gut separiert. Das nennt man Klassifikation und die dazugehörige Gerade Klassifikationsgerade. Natürlich kann es auch passieren, dass die Eingabepunkte nicht linear trennbar sind. Die beste Klassifikationsgerade erhält man als Lösung eines konvexen Optimierungsproblems.\n",
    "Jede Gerade in der Ebene kann dargestellt werden als\n",
    "\\begin{align*}\n",
    "\\{x \\in \\mathbb{R}^2|w^Txb = 0\\}\n",
    "\\end{align*}\n",
    "wobei $w \\in \\mathbb{R}^2$ der Normalenvektor und $b \\in \\mathbb{R}$ der Offset der Geraden darstellt (s. Hesse-Normalform). Der Vektor $w$ muss in der Darstellung nicht normiert sein.\n",
    "Das dazugehoerige Optimierungsproblem ist das folgende:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{w,b} \\quad&\\frac{1}{2}w^Tw+c\\sum^k_{i=1}\\xi_i\\\\\n",
    "s.t. \\quad& y_i(w^Tx_i+b)\\geq 1 - \\xi_i\\\\\n",
    "& \\xi_i \\ge 0,\n",
    "\\end{align*}\n",
    "\n",
    "wobei $c \\in \\mathbb{R}$ ein fest vorgegebener Parameter (Regularisierungsparameter) ist und $\\xi_i \\in \\mathbb{R}$ der\n",
    "Klassifikationsfehler des Punktes $x_i$.\n",
    "\n",
    "Man kann hier wiederum den Klassifikationsfehler unterschiedlich bestrafen. Eine Variante bestraft den Fehler linear, eine zweite Variante bestraft den Fehler quadratisch und eine dritte Variante logistisch. Sie sollen alle drei Varianten implementieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Linearer Fehler:\n",
    "Hier wird der Klassifikationsfehler $\\xi_i$ linear bestraft. Dies heißt $L^1$-Loss SVM.Das dazugehörige Optimierungsproblem ist das folgende:\n",
    "\\begin{equation}\n",
    "\\tag{P1}\\min_{w,b} \\frac{1}{2}w^Tw + c \\cdot \\sum^k_{i=1}\\max\\{0, 1 - y_i (w^Tx_i+b)\\}\n",
    "\\end{equation}\n",
    "Bestimmt man von dem Optimierungsproblem  (P1) die optimale Lösung $(w^*, b^*)$, dann ist die beste Klassifikationsgerade gegeben durch Gleichung (*) und der dazugehörige Klassifikator für einen Punkt $x_i$ als\n",
    "$\n",
    "sign((w^*)^Tx+b^*)\n",
    "$\n",
    "##### Vektorisierung\n",
    "\\begin{gather}\n",
    "\\frac{1}{2}w^Tw + c \\cdot \\sum^k_{i=1}\\max\\{0, 1 - y_i (w^Tx_i+b)\\}\n",
    "= \\frac{1}{2}z^TAz + c \\cdot \\sum^k_{i=1}\\max\\{0, 1 - y_i \\langle u_i, z \\rangle\\} \\\\\n",
    "\\text{mit\n",
    "$\\quad A =  \\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{pmatrix}$,$\\quad z =  \\begin{pmatrix} \n",
    "b \\\\\n",
    "w_1 \\\\\n",
    "w_2 \n",
    "\\end{pmatrix}\\quad$ und $\\quad u_i = \\begin{pmatrix} \n",
    "1 \\\\\n",
    "x_{1,i} \\\\\n",
    "x_{2,i}\n",
    "\\end{pmatrix}$}\n",
    "\\end{gather}\n",
    "Das zueghoerige Optimierungsproblem ist nun:\n",
    "\\begin{equation}\n",
    "\\min_z \\frac{1}{2}\\langle z,z \\rangle_A + c \\cdot \\sum^k_{i=1}\\max\\{0, 1 - y_i \\langle u_i, z \\rangle\\}\n",
    "\\end{equation}\n",
    "bzw. Zielfunktion:\n",
    "\\begin{equation}\n",
    "f(z) =\\frac{1}{2}\\langle z,z \\rangle_A + c \\cdot \\sum^k_{i=1}\\max\\{0, 1 - y_i \\langle u_i, z \\rangle\\}\n",
    "\\end{equation}\n",
    "\n",
    "##### Ableitung der Zielfunktion:\n",
    "\\begin{align*}\n",
    "h_i(z) &= \\max\\{0, 1 - y_i \\langle u_i, z \\rangle\\}\\\\\n",
    "g_i(z) &= 1 - y_i \\langle u_i, z \\rangle\\\\\n",
    "\\nabla g_i(z) &= - y_iu_i \\\\\n",
    "\\nabla h_i(z) &= \n",
    "\\begin{cases}\n",
    "\\nabla g_i(z) &\\quad\\text{if}\\quad g_i(z) > 0\\\\\n",
    "t\\cdot \\nabla g_i(z), t\\in[0,1] &\\quad\\text{if}\\quad g_i(z) = 0\\\\\n",
    "0 &\\quad\\text{sonst}\n",
    "\\end{cases}\\\\\n",
    "\\nabla f(z) & =  Az + c \\cdot \\sum^k_{i=1} \\nabla h_i(z)\\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_linloss(z, **kwargs):\n",
    "    A = kwargs.get(\"A\",np.array([[1, 0, 0],[0, 1, 0],[0, 0, 0]]))\n",
    "    u = kwargs.get(\"design_mat\",np.array([0]))\n",
    "    y = kwargs.get(\"y\",np.array([0]))\n",
    "    c = kwargs.get(\"c\",10)\n",
    "    result = 0.5 * z.T.dot(A).dot(z)\n",
    "    sum_e = 0\n",
    "    for i in range(0, len(y)):\n",
    "        sum_e += max(0, 1 - y[i] * np.array([u[i]]).dot(z))\n",
    "    result += c * sum_e\n",
    "    return result\n",
    "        \n",
    "def df_linloss(z, **kwargs):\n",
    "    A = kwargs.get(\"A\",np.array([[1, 0, 0],[0, 1, 0],[0, 0, 0]]))\n",
    "    u = kwargs.get(\"design_mat\",np.array([0]))\n",
    "    y = kwargs.get(\"y\",np.array([0]))\n",
    "    c = kwargs.get(\"c\",10)\n",
    "    result = A.dot(z)\n",
    "    sum_e = np.zeros(3).reshape(z.shape)\n",
    "    for i in range(0, len(y)):\n",
    "        if 1 - y[i] * np.array([u[i]]).dot(z) > 0:\n",
    "            sum_e -= y[i] * np.array([u[i]]).T       \n",
    "    result += c * sum_e\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quadratischer Fehler\n",
    "Hier wird der Klassifikationsfehler $\\xi_i$ quadratisch bestraft. Dies heißt $L^2$-Loss SVM. Das dazugehörige Optimierungsproblem ist das folgende:\n",
    "\\begin{align*}\n",
    "\\tag{P2}\\min_{w,b} \\frac{1}{2}w^Tw + c \\cdot \\sum^k_{i=1}\\left[\\max\\{0, 1 - y_i (w^Tx_i+b)\\}^2\\right]\n",
    "\\end{align*}\n",
    "\n",
    "##### Vektorisierung\n",
    "\\begin{gather}\n",
    "\\frac{1}{2}w^Tw + c \\cdot \\sum^k_{i=1}\\max\\{0, 1 - y_i (w^Tx_i+b)\\}^2\n",
    "= \\frac{1}{2}z^TAz + c \\cdot \\sum^k_{i=1}\\max\\{0, 1 - y_i \\langle u_i, z \\rangle\\}^2 \\\\\n",
    "\\text{mit\n",
    "$\\quad A =  \\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{pmatrix}$,$\\quad z =  \\begin{pmatrix} \n",
    "b \\\\\n",
    "w_1 \\\\\n",
    "w_2 \n",
    "\\end{pmatrix}\\quad$ und $\\quad u_i = \\begin{pmatrix} \n",
    "1 \\\\\n",
    "x_{1,i} \\\\\n",
    "x_{2,i}\n",
    "\\end{pmatrix}$}\n",
    "\\end{gather}\n",
    "Das zueghoerige Optimierungsproblem ist nun:\n",
    "\\begin{equation}\n",
    "\\min_z \\frac{1}{2}\\langle z,z \\rangle_A + c \\cdot \\sum^k_{i=1}\\max\\{0, 1 - y_i \\langle u_i, z \\rangle\\}^2\n",
    "\\end{equation}\n",
    "bzw. Zielfunktion:\n",
    "\\begin{equation}\n",
    "f(z) =\\frac{1}{2}\\langle z,z \\rangle_A + c \\cdot \\sum^k_{i=1}\\max\\{0, 1 - y_i \\langle u_i, z \\rangle\\}^2\n",
    "\\end{equation}\n",
    "\n",
    "##### Ableitung\n",
    "\\begin{gather}\n",
    "\\frac{\\partial f(z)}{\\partial z} &= \\frac{\\partial}{\\partial z} \\left[\\frac{1}{2}\\langle z,z \\rangle_A + c \\cdot \\sum^k_{i=1}\\max\\{0, 1 - y_i \\langle u_i, z \\rangle\\}^2\\right]\\\\\n",
    "&=  Az + c \\cdot \\sum^k \\left[ 2 \\cdot \\max\\{0, 1 - y_i \\langle u_i, z \\rangle\\} \\cdot \\left( -y_i u_i\\right)\\right]\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_quadloss(z, **kwargs):\n",
    "    A = kwargs.get(\"A\",np.array([[1, 0, 0],[0, 1, 0],[0, 0, 0]]))\n",
    "    u = kwargs.get(\"design_mat\",np.array([0]))\n",
    "    y = kwargs.get(\"y\",np.array([0]))\n",
    "    c = kwargs.get(\"c\",10)\n",
    "    result = 0.5 * z.T.dot(A).dot(z)\n",
    "    sum_e = 0\n",
    "    for i in range(0, len(y)):\n",
    "        sum_e += max(0, 1 - y[i] * np.array([u[i]]).dot(z))**2\n",
    "    result += c * sum_e\n",
    "    return result\n",
    "        \n",
    "def df_quadloss(z, **kwargs):\n",
    "    A = kwargs.get(\"A\",np.array([[1, 0, 0],[0, 1, 0],[0, 0, 0]]))\n",
    "    u = kwargs.get(\"design_mat\",np.array([0]))\n",
    "    y = kwargs.get(\"y\",np.array([0]))\n",
    "    c = kwargs.get(\"c\",10)\n",
    "    result = A.dot(z)\n",
    "    sum_e = np.zeros(3).reshape(z.shape)\n",
    "    for i in range(0, len(y)):\n",
    "        temp = 1 - y[i] * np.array([u[i]]).dot(z)\n",
    "        if temp > 0:\n",
    "            sum_e -= 2*temp*y[i] * np.array([u[i]]).T       \n",
    "    result += c * sum_e\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistischer Fehler:\n",
    "Hier wird der Klassifikationsfehler $\\xi_i$ durch die logistische Funktion bestraft. Dies heißt Logistic Regression SVM. Das dazugehörige Optimierungsproblem ist das folgende:\n",
    "\\begin{align*}\n",
    "\\tag{P2}\\min_{w,b} \\frac{1}{2}w^Tw + c \\cdot \\sum^k_{i=1}\\left[\\ln(1 + e^{-y_i(w^Tx_i+b)})\\right]\n",
    "\\end{align*}\n",
    "\n",
    "##### Vektorisierung\n",
    "\\begin{align*}\n",
    "\\frac{1}{2}w^Tw + c \\cdot \\sum^k_{i=1}\\left[\\ln(e^{-y_i(w^Tx_i+b)})\\right] = \\frac{1}{2}\\langle z, z\\rangle_A + c \\cdot \\sum^k_{i=1}\\left[\\ln(1 + e^{-y_i\\langle u_i, z \\rangle})\\right]\\\\\n",
    "\\text{mit\n",
    "$\\quad A =  \\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{pmatrix}$,$\\quad z =  \\begin{pmatrix} \n",
    "b \\\\\n",
    "w_1 \\\\\n",
    "w_2 \n",
    "\\end{pmatrix}\\quad$ und $\\quad u_i = \\begin{pmatrix} \n",
    "1 \\\\\n",
    "x_{1,i} \\\\\n",
    "x_{2,i}\n",
    "\\end{pmatrix}$}\n",
    "\\end{align*}\n",
    "\n",
    "##### 1te Ableitung\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla f(z)&= \\frac{1}{2}\\langle z, z\\rangle_A + c \\cdot \\sum^k_{i=1}\\left[\\ln(1 + e^{-y_i\\langle u_i, z \\rangle})\\right]\\\\\n",
    "&=  Az + c \\cdot \\sum^k_{i=1}\\left[\\frac{1}{1 + e^{-y_i\\langle u_i, z \\rangle }} \\cdot e^{-y_i\\langle u_i, z \\rangle } \\cdot -y_iu_i \\right]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "##### 2te Ableitung\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla^2 f(z) &= \\nabla \\left[ Az + c \\cdot \\sum^k_{i=1}\\left[\\frac{1}{1 + e^{-y_i\\langle u_i, z \\rangle }} \\cdot e^{-y_i\\langle u_i, z \\rangle } \\cdot -y_iu_i \\right]\\right]\\\\\n",
    "&= A^T + c \\cdot \\sum^k_{i=1}-y_iu_i \\left[\\frac{-y_iu_i}{1 + e^{-y_i\\langle u_i, z \\rangle }} \\cdot e^{-y_i\\langle u_i, z \\rangle} + \\frac{y_iu_i}{(1 + e^{-y_i\\langle u_i, z \\rangle })^2} \\cdot e^{-y_i\\langle u_i, z \\rangle} \\cdot (1+ e^{-y_i\\langle u_i, z \\rangle})\\right]\\\\\n",
    "&= A^T = A\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_logloss(z, **kwargs):\n",
    "    A = kwargs.get(\"A\",np.array([[1, 0, 0],[0, 1, 0],[0, 0, 0]]))\n",
    "    u = kwargs.get(\"design_mat\",np.array([0]))\n",
    "    y = kwargs.get(\"y\",np.array([0]))\n",
    "    c = kwargs.get(\"c\",10)\n",
    "    result = 0.5 * z.T.dot(A).dot(z)\n",
    "    sum_e = 0\n",
    "    for i in range(0, len(y)):\n",
    "        sum_e += max(0, 1 - y[i] * np.array([u[i]]).dot(z))**2\n",
    "    result += c * sum_e\n",
    "    return result\n",
    "        \n",
    "def df_logloss(z, **kwargs):\n",
    "    A = kwargs.get(\"A\",np.array([[1, 0, 0],[0, 1, 0],[0, 0, 0]]))\n",
    "    u = kwargs.get(\"design_mat\",np.array([0]))\n",
    "    y = kwargs.get(\"y\",np.array([0]))\n",
    "    c = kwargs.get(\"c\",10)\n",
    "    result = A.dot(z)\n",
    "    sum_e = np.zeros(3).reshape(z.shape)\n",
    "    for i in range(0, len(y)):\n",
    "        temp = 1 - y[i] * np.array([u[i]]).dot(z)\n",
    "        if temp > 0:\n",
    "            sum_e -= 2*temp*y[i] * np.array([u[i]]).T       \n",
    "    result += c * sum_e\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Aufgabe:\n",
    "Schreiben Sie SVM-Solver für alle drei beschriebenen Fehler. \n",
    "\n",
    "- Ihr Programm sollte als erstes die Punkte von der Grafikoberfläche in zwei Gruppen einlesen. \n",
    "- Danach sollen dann, abhängig vom Fehler (linearer Fehler, quadratischer Fehler oder logistischer Fehler), alle drei entsprechenden Optimierungsprobleme gelöst werden. \n",
    "- Danach soll dann die Klassifikationsgerade dargestellt werden. Den Regularisierungsparameter $c$ können Sie in allen drei Varianten fest auf den Wert 10 setzen.\n",
    "- Beachten Sie, dass die drei verschiedenen Optimierungsprobleme in unterschiedliche Klassen von Funktionen fallen (stetig sind sie alle, konvex auch). Benutzen Sie also für jedes Optimierungsproblem einen jeweils effizienten Algorithmus. Die normale Subgradienten-Methode klappt natürlich immer, da sie die wenigsten Voraussetzungen an das Optimierungsproblem stellt. Nur ist diese nicht immer am effizientesten.\n",
    "- Sie sollen außerdem die passenden Algorithmen allgemeingültig implementieren, so dass sie auch für beliebige andere Funktionen wiederverwendet werden können. Die passenden Funktionen, (Sub-)gradienten oder zweiten Ableitungen sollen jeweils als Funktionshandle übergeben werden.\n",
    "- Die Optimierungsalgorithmen sollen pro Iteration die bisherige Gesamtzahl der Funktionsorakelanfragen, die Schrittlänge $\\sigma_k$, Informationen über den aktuellen Funktionswert und die $\\mathcal{l}_\\infty$-norm des Gradienten ausgeben.\n",
    "- Des Weiteren sollen Ihre Optimierungsalgorithmen passende numerische Tests für Gradienten und Hesse-Matrizen beinhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sub_gradient(d_func, x_k, **kwargs):\n",
    "    x_list = list()\n",
    "    eps = kwargs.get('eps',0.01)\n",
    "    max_k = kwargs.get('max_k',100)\n",
    "    exit = 1\n",
    "    gradient_k = d_func(x_k, **kwargs)\n",
    "    d_k = -gradient_k/np.linalg.norm(gradient_k, ord = 2)\n",
    "    k = 0\n",
    "    sigma = 1\n",
    "    while gradient_k.any():\n",
    "        x_list.append(x_k)\n",
    "        sigma_k =  sigma/(k+1)**(1/2)\n",
    "        x_k1 = x_k + sigma_k * d_k\n",
    "        if k == max_k:\n",
    "            exit = 2\n",
    "            break\n",
    "        elif np.linalg.norm(x_k1 - x_k, ord = 2)\\\n",
    "            <= eps * np.maximum(1, np.linalg.norm(x_k, ord = 2)):\n",
    "            exit = 3\n",
    "            break\n",
    "        elif np.linalg.norm(gradient_k, ord = 2) <= eps:\n",
    "            exit = 4\n",
    "            break\n",
    "        gradient_k1 = d_func(x_k1, **kwargs)\n",
    "        d_k = -gradient_k1/np.linalg.norm(gradient_k1, ord = 2)\n",
    "        gradient_k = gradient_k1\n",
    "        x_k = x_k1\n",
    "        k += 1\n",
    "    return (x_list, k, x_k, exit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def armijo(func, dfunc, x_k, **kwargs):\n",
    "    # 1.Schritt\n",
    "    sigma = kwargs.get('a_sigma_o',1)\n",
    "    k = 0\n",
    "    delta = kwargs.get('a_delta',0.001)\n",
    "    beta_1 = kwargs.get('a_beta_2',0.5)\n",
    "    beta_2 = kwargs.get('a_beta_1',0.5)\n",
    "    max_k = kwargs.get('max_k',1000)\n",
    "    fk = func(x_k, **kwargs)\n",
    "    gradient_k = dfunc(x_k, **kwargs)\n",
    "    d_k = -gradient_k\n",
    "    sigma = 1\n",
    "    fk1 = func(x_k+sigma*d_k, **kwargs)\n",
    "    # 2.Schritt\n",
    "    condition = delta * gradient_k.T.dot(d_k)\n",
    "    while fk1 > fk + condition * sigma and k < max_k:\n",
    "        # 3.Schritt\n",
    "        sigma *= beta_1\n",
    "        fk1 = func(x_k + sigma * d_k, **kwargs)\n",
    "        # 4.Schritt\n",
    "        k = k+1\n",
    "    return sigma  \n",
    "\n",
    "def gradient(func, d_func, x_k, **kwargs):\n",
    "    x_list = list()\n",
    "    eps_1 = kwargs.get('eps',0.0001)\n",
    "    max_k = kwargs.get('max_k',1000)\n",
    "    exit = 1\n",
    "    eps_2 = np.sqrt(eps_1)\n",
    "    eps_3 = np.sqrt(eps_1)\n",
    "    f_k = func(x_k, **kwargs)\n",
    "    k = 0 #Schritt 1 (Startpunkt wird der Funktion mitgegeben)\n",
    "    gradient_k = d_func(x_k, **kwargs)\n",
    "    while gradient_k.any(): # Schritt 2\n",
    "        #Schritt 3\n",
    "        x_list.append(x_k)\n",
    "        gradient_k = d_func(x_k, **kwargs)\n",
    "        d_k = -gradient_k\n",
    "        sigma_k = armijo(func, d_func, x_k, **kwargs)\n",
    "        x_k1 = x_k + sigma_k * d_k    \n",
    "        \n",
    "        # Weitere Abrruchkriterien\n",
    "        f_k1 = func(x_k1, **kwargs)\n",
    "        if k == max_k:\n",
    "            exit = 2\n",
    "            break\n",
    "        if np.fabs(f_k - f_k1)\\\n",
    "            <= eps_1 * np.maximum(1, np.fabs(f_k)):\n",
    "            exit = 3\n",
    "            break\n",
    "        elif np.linalg.norm(x_k1 - x_k, ord = 2)\\\n",
    "            <= eps_2 * np.maximum(1, np.linalg.norm(x_k, ord = 2)):\n",
    "            exit = 4\n",
    "            break\n",
    "        elif np.linalg.norm(gradient_k)\\\n",
    "            <= eps_3 * np.maximum(1, np.fabs(f_k)):\n",
    "            exit = 5\n",
    "            break\n",
    "\n",
    "        x_k = x_k1 \n",
    "        f_k = f_k1\n",
    "        #Schritt 4\n",
    "        k += 1\n",
    "    return (x_list, k, x_k, exit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1_lab1 = np.array([-10,-1,-30,-20])\n",
    "x2_lab1 = np.array([1,3,4,2])\n",
    "\n",
    "x1_lab2 = np.array([1,30,60,20])\n",
    "x2_lab2 = np.array([6,3,3,1])\n",
    "y = np.array([1,1,1,1,-1,-1,-1,-1])\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10  -1 -30 -20   1  30  60  20]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sub_grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6cb64d382549>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_offset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_linloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesign_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1e-20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sub_grad' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "x1 = np.concatenate((x1_lab1,x1_lab2))\n",
    "x2 = np.concatenate((x2_lab1,x2_lab2))\n",
    "print(x1)\n",
    "x_offset = np.ones(len(x1)) \n",
    "x = np.array([x1,x2,x_offset])\n",
    "x0 = np.zeros(3).reshape(3,1)\n",
    "ret = sub_grad(x0, df_linloss, design_mat = x.T, y=y, c=1*1e-20)\n",
    "w = np.array([ret[0],ret[1]])\n",
    "b = ret[2]\n",
    "t = np.arange(-20,20,1)\n",
    "plt.plot(x1_lab1,x2_lab1,'ro')\n",
    "plt.plot(x1_lab2,x2_lab2,'bo')\n",
    "\n",
    "x0 = np.zeros(3).reshape(3,1)\n",
    "ret = sub_grad(x0, df_linloss, design_mat = x.T, y=y, c=10)\n",
    "w = np.array([ret[0],ret[1]])\n",
    "b = ret[2]\n",
    "\n",
    "plt.plot(t,-w[0]/w[1]*t-b/w[1],'b')\n",
    "\n",
    "\n",
    "x0 = np.zeros(3).reshape(3,1)\n",
    "ret = sub_gradient(df_linloss, x0, design_mat = x.T, y=y, c=10)\n",
    "w = np.array([ret[2][0],ret[2][1]])\n",
    "b = ret[2][2]\n",
    "\n",
    "plt.plot(t,-w[0]/w[1]*t-b/w[1],'purple')\n",
    "plt.ylim( (-10,10) )\n",
    "plt.xlim( (-5,15) )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x1 = np.concatenate((x1_lab1,x1_lab2))\n",
    "x2 = np.concatenate((x2_lab1,x2_lab2))\n",
    "x_offset = np.ones(len(x1)) \n",
    "x = np.array([x1,x2,x_offset])\n",
    "x0 = np.zeros(3).reshape(3,1)\n",
    "w = np.array([ret[0],ret[1]])\n",
    "b = ret[2]\n",
    "t = np.arange(-20,20,1)\n",
    "plt.plot(x1_lab1,x2_lab1,'ro')\n",
    "plt.plot(x1_lab2,x2_lab2,'bo')\n",
    "\n",
    "x0 = np.zeros(3).reshape(3,1)\n",
    "ret = sub_grad(x0, df_linloss, design_mat = x.T, y=y, c=10)\n",
    "w = np.array([ret[0],ret[1]])\n",
    "b = ret[2]\n",
    "\n",
    "plt.plot(t,-w[0]/w[1]*t-b/w[1],'b')\n",
    "\n",
    "\n",
    "x0 = np.zeros(3).reshape(3,1)\n",
    "ret = gradient(f_quadloss, df_quadloss, x0, design_mat = x.T, y=y, c=0.01)\n",
    "w = np.array([ret[2][0],ret[2][1]])\n",
    "b = ret[2][2]\n",
    "plt.plot(t,-w[0]/w[1]*t-b/w[1],'yellow')\n",
    "plt.ylim( (-10,10) )\n",
    "plt.xlim( (-5,15) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
