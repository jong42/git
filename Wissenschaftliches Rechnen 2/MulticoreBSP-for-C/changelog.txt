
1.2.0:
	-Revamped internal pinning strategies; now automatically
	 constructs sensible pinning arrays for nested BSP runs.

	-Better initial data locality of the communication threads
	 of the different BSP threads.

	-Templated BSP communication primitives are now available
	 for C++. To enable, simply include mcbsp-templates.hpp
	 When pointers to source and destination memory areas are
	 of type T*, then offset and sizes should be given as the
	 number of elements of type T (instead of raw bytes).

	 The translation from templated primitives to regular
	 BSP primitives happens at compile time; there is no
	 performance penalty.
	 If any of the pointers to source or destination areas
	 are of type (void*), then BSP communication reverts to
	 the default behaviour: offset and size parameters must
	 then be given in bytes.

	                        ***NOTE***
	     There is no templated bsp_hpmove as its intended
	 use requires casting the retrieved pointers into the BMSP
	   receive-buffer into the appropriate T*. Additionally,
	  the type of T is typically only known after inspecting
	    the tag; which is retrieved during the same call to
	                       bsp_hpmove.

	-bsp_abort and bsp_vabort now have proper const modifiers.

	-Improved documentation.

	-The pinning functionality supplied by mcutil.h/mcutil.c
	 now returns the pinning list in size_t instead of int.

	-Maps in mcutil.h/mcutil.c now have size_t values
	 instead of unsigned long ints.

	-Bugfix: bsp_get_tag now correctly sets the memory pointed
	         to by STATUS to the payload size.
	 Thanks to Jing Fan (fanjing09@gmail.com, University of
	 Wisconsin-Madison) for reporting this bug.

	-Bugfix: nested BSP sections using the C++ wrapper now
	         works.

	-Bugfix: no longer a possibility of a race condition 
		 occuring with nested C++ calls and direct_gets.

	-Bugfix: fixed error message for bsp_get on unregistered
	         memory areas.

	-Bugfix: fixed error in integer-compare function, fixed
	         conversion issues, and fixed other issues 
		 detected by clang version 3.3. Fixed file handle
		 never closed in mcutil.c.
	Thanks to Joshua Moerman (joshua@keesmoerman.nl) for
	reporting these issues.

	-Compiles without warnings with GCC 4.8.2, GCC 4.4.7,
	 clang 3.3, and clang 3.4. Tested on the following
	 platforms:
	  * 64-bit linux, AMD Phenom II with clang 3.3
	  * 64-bit linux, AMD Phenom II with GCC 4.8.2
	  * 64-bit linux, Intel Sandy Bridge with clang 3.4
	  * 64-bit linux, Intel Sandy Bridge with GCC 4.8.2
	  * Darwin 10.8 (OS X), Intel Merom, with GCC 4.2.1

1.1.0:

	-Now includes various ways to influence thread pinning.
	 In order of priority:

	 	* Code-based: the following six functions influence
		  the pinning strategy of MulticoreBSP:

		  -mcbsp_set_maximum_threads
		  -mcbsp_set_available_cores
		  -mcbsp_set_threads_per_core
		  -mcbsp_set_thread_numbering
		  -mcbsp_set_affinity_mode
		  -mcbsp_set_pinning
		  -mcbsp_set_reserved_cores

		  See the documentation or the below file-based
		  description for details on the different settings.
		  To use these functions, the user should include
		  the `mcbsp-affinity.h' header.

		  Options set by using any of these functions can
		  only be overruled by later calls to the same 
		  functions. Default values are found below.
	
		* File-based: write architecture info in `machine.info'
		  The advantage is that MulticoreBSP users need not
		  know the hardware specifics; a system administrator
		  can simply provide a single text file to the users
		  of his machine, if the MulticoreBSP defaults do not
		  suffice.

		  Example machine.info (excluding bracket lines):
			{
				threads 4
				cores 61
				threads_per_core 4
				thread_numbering consecutive
				affinity manual
				pinning 4 8 12 16
				reserved_cores 0
			}

		  -threads should be a positive integer, and equals
		   the maximum number of threads MulticoreBSP can 
		   spawn.
		   
		  -cores should be a positive integer, and equals
		   the number of cores available on the machine.

		  -threads_per_core should be a positive integer.
		   Indicates how many hardware threads map to the
		   same (hardware) CPU core. Normally this is 1;
		   but machines with hyper-threading enabled have 
		   2 threads_per_core, while the Xeon Phi, for
		   example, has 4 hardware threads per core.

		                   ***WARNING***
		    taking threads > cores*threads_per_core is 
		     possible, but may require manual pinning

		  -thread_numbering is either `consecutive' or
		   `wrapped', and only has effect if the number of
		   threads_per_core is larger than 1.
		   
		   Let i, k be integers with
		        0 <= i < cores, and
			0 <= k < threads_per_core.

		   With `wrapped', thread number (i + k * cores)
		   maps to core i.

		   With `consecutive', thread number (k + i *
		   threads_per_core) maps to core i.

		   Whether your machine uses wrapped or consecutive
		   numbering depends on the operating system and on
		   the hardware; e.g., the Linux kernel on hyper-
		   treading architectures uses a wrapped thread 
		   numbering.
		   On the other hand, the Intel Xeon Phi OS, for
		   example, numbers its threads consecutively.

		  -affinity is either scatter, compact, or manual.
		   Scatter will spread MulticoreBSP threads as much
		   as possible over all available cores, thus
		   maximising bandwidth use on NUMA systems.
		   Compact will pin all MulticoreBSP threads as
		   close to each other as possible.
		   Both schemes rely on the thread numbering being
		   set properly. Manual will let the i-th BSP
		   thread be pinned to the hardware thread with OS
		   number pinning[i], where `pinning' is a user-
		   supplied array with length equal to the number
		   of threads.

		  -pinning is a list of positive integers of length
		   threads. Pinning is mandatory when affinity is
		   manual, otherwise it will be ignored.

		  -reserved_cores An array with elements i in the
		   range 0 <= i < cores. These cores will not be
		   used by BSP threads. Useful in situations where 
		   part of the machine is reserved for dedicated 
		   use, such as for OS-use or for communication
		   handling.

		 * Code-based defaults: the following fields (as
		   declared in mcbsp-affinity.h) define default
		   settings that can be changed at run-time:
		   
		   -MCBSP_DEFAULT_AFFINITY
		   -MCBSP_DEFAULT_THREADS_PER_CORE
		   -MCBSP_DEFAULT_THREAD_NUMBERING

		   Other defaults are fixed to the below values.
		   WARNING: as per the ordering in priority, the 
		   `machine.info' file, if it exists, will 
		   always override these defaults, even if these 
		   were changed at run-time. Use the explicit
		   code-based setters when the file-based 
		   defaults are to be overridden.

		 * Defaults: if no `machine.info' file is found,
		   or if this file leaves some options undefined,
		   and if the corresponding option was not set 
		   manually, then the following default values
		   will be used. Some of these default values
		   may be adapted (see above).

		   -threads: <the number of OS hardware threads>
		   -cores: <the number of OS hardware threads>
		   -threads_per_core: 1
		   -thread_numbering: consecutive
		   -affinity: scatter
		   -reserved_cores: <empty array>

		                     ***NOTE***
		         THESE DEFAULTS ARE NOT SUITABLE FOR
		              HYPER-THREADING MACHINES

	-When compiling with the MCBSP_SHOW_PINNING macro defined,
	 each bsp_begin() will print pinning data to stdout.

	-Fixed likely overflow issue in examples/parallel_loop.c
	 when the number of spawned threads is high.

	-Fixed compiler warning for Mac OS X.

	-Now compiles for Windows using MinGW 4.7.2 and MinGW 4.4.6.
	 For 4.4.6 and C++ support, remove the -ansi and -std flags
	 from CPPFLAGS in include.mk. This is a MinGW bug.

	 Windows support does not require POSIX real-time extensions,
	 instead MulticoreBSP uses the Windows API for high-resolution
	 timing. Pinning support is complete, but necessarily occurs
	 after thread creation; it is unclear if the Windows scheduler
	 immediately pins the thread or whether there is some latency
	 involved. This is also true for the thread with BSP ID zero
	 on Linux machines.

	-Fixed errors in documentation.

	-Fixed deadlock that previously might occur on bsp_abort.

	-Fixed registration / de-registration of NULL addresses.

	-Fixed potential memory leaks tied to using bsp_hpmove().

	-Fixed bug in hierarchical execution support, brought to
	 light by the new examples/hierarchical.c example.
	
	-Fixed bug where bsp_nprocs could malfunction outside of
	 SPMD areas if bsp_init was not called previously.

	-Increased efficiency of all BSMP and DRMA communication.

	-Support for synchronisation via spin locks instead of
	 mutexes. Undefine the MCBSP_USE_SPINLOCK macro to use
	 PThreads mutexes instead of the now-default spinlocks.

	-More guards against local copies or remote access via
	 DRMA primives involving unregistered memory areas.

	-The C++ wrapper now provides the SIZE_MAX macro if it did
	 not already exist. This provides the maximum value for
	 values with type size_t.

	-bsp_hpsend directive added. Semantics are like that of
	 the bsp_send, but there is no buffer-on-send. The tag and
	 payload memory areas thus must remain unchanged until the
	 end of the next bsp_sync. See documentation for details.

1.0.1:

	-Now includes a wrapper for easier use with C++, see mcbsp.hpp
	-Added a collection of simple examples to the codebase
	-Bugfix for when main is the implied SPMD function (tests/spmd.c)
	-Now compiles and links to BSP applications without warnings on 
	 recent AMD and Intel processors using GCC versions 4.7 and 4.8.
	-Now compiles with the XCode-supplied GCC version 4.2.1 on
	 Mac OS X. This required two changes to the source code:
	 
	 1. Mach-specific timers have been substituted for timers based
	 on POSIX Realtime (rt) extensions as the latter is unavailable 
	 on Mac OS X.
	 
	 2. Thread pinning is not available on Mac OS X. Instead, OS X
	 supports so-called thread affinity sets; see

	     https://developer.apple.com/library/mac/#releasenotes/
		Performance/RN-AffinityAPI/_index.html

	 MulticoreBSP for C therefore does NOT support pinning on OS X,
	 but instead uses (or exposes, in case of a manual affinity
	 strategy) the Mac OS X affinity API.

	 Important consequences:

		*OS X affinity policy directives are basically HINTS;
	 	 the OS X kernel may decide to ignore them. In
		 particular, it may decide to migrate threads while
		 the parallel application is running.

		*While the compact and scatter strategies translate
		 succesfully to the OS X affinity-sets paradigm,
		 manual thread pinning strategies might not.
		 Therefore, when running MulticoreBSP for C programs
		 on Mac OS X which use manually set affinities,
		 please make sure your thread `pinnings' still make
		 sense within the OS X affinity-sets formulation.

